{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker TorchPoints3d\n",
    "\n",
    "This notebook is for creating Amazon SageMaker training and inference containers for PyTorch Points 3D. \n",
    "\n",
    "Getting Started:\n",
    "\n",
    "1. Clone the PyTorch Points 3D [GitHub repoistory](https://github.com/nicolas-chaulet/torch-points3d) and download this notebook into the working directory.\n",
    "\n",
    "```\n",
    "$ git clone https://github.com/nicolas-chaulet/torch-points3d\n",
    "```\n",
    "\n",
    "`NOTE`: This code is under active development, tested July 22 commit `12dfca3e94add3981f7f37db25df1e5acee640fe`\n",
    "\n",
    "This notebook will take you through the following steps:\n",
    "\n",
    "1. Build and register Training Containers\n",
    "2. Build and register Inference Container\n",
    "3. Train model\n",
    "4. Run Inference\n",
    "5. Visualize results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset\n",
    "\n",
    "Download the `shapenet` dataset, should take about 1 minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --no-check-certificate \"https://shapenet.cs.stanford.edu/media/shapenetcore_partanno_segmentation_benchmark_v0_normal.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upzip the dataset, should take about 20s, size should be 2.8GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "!mkdir -p dataset/raw\n",
    "!unzip -q shapenetcore_partanno_segmentation_benchmark_v0_normal.zip -d ./dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -h dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the torchpoints3d dataset to which you upload the `shapnet/raw` folder, should take about 3mins to upload 16k files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker import session\n",
    "\n",
    "s3_shapenet_uri = 's3://{}/torchpoints3d'.format(session.Session().default_bucket())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "!aws s3 sync dataset/shapenetcore_partanno_segmentation_benchmark_v0_normal $s3_shapenet_uri/shapenet/raw --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that we have some files uploaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls $s3_shapenet_uri/shapenet/raw/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test training container\n",
    "\n",
    "You can test the training container by running an interactive docker container and attaching the downloaded dataset\n",
    "\n",
    "```\n",
    "$ docker run -it --mount src=\"$(pwd)/dataset/shapenetcore_partanno_segmentation_benchmark_v0_normal\",target=\"/opt/ml/input/data/training/shapenet/raw\",type=bind sagemaker-torchpoints3d-training:latest\n",
    "```\n",
    "\n",
    "Once in the container, create the output directory and run the training script\n",
    "\n",
    "```\n",
    "$ cd /opt/ml/code\n",
    "$ mkdir -p /opt/ml/model /opt/ml/output/data\n",
    "$ SM_MODEL_DIR=/opt/ml/model SM_OUTPUT_DATA_DIR=/opt/ml/output/data SM_CHANNEL_TRAINING=/opt/ml/input/data/training python sagemaker_train.py --epochs 3\n",
    "```\n",
    "\n",
    "This will run for a few short epochs, and write the output model to `/opt/ml/model` and training logs to `/opt/ml/output/data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the zip and dataset folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm shapenetcore_partanno_segmentation_benchmark_v0_normal.zip && rm -Rf dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train 3d Point cloud\n",
    "\n",
    "Train a 3d point cloud for the `shapenet` dataset\n",
    "\n",
    "`local_estimator.fit()` will run the training job on the jupyter notebook and `estimator.fit()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region =  boto3.session.Session().region_name\n",
    "role = get_execution_role()\n",
    "\n",
    "training_image = '{}.dkr.ecr.{}.amazonaws.com/sagemaker-torchpoints3d-training:latest'.format(account_id, region)\n",
    "\n",
    "hyperparameters = {\"epochs\": 100,\n",
    "                   \"lr\": 0.01}\n",
    "\n",
    "estimator = Estimator(training_image,\n",
    "                      role=role,\n",
    "                      train_instance_count=1,\n",
    "                      train_instance_type='ml.p3.2xlarge',\n",
    "                      image_name=training_image,\n",
    "                      hyperparameters=hyperparameters)\n",
    "\n",
    "estimator.fit(s3_shapenet_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the training job model archive and list the contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $estimator.model_data .\n",
    "!mkdir -p model && tar -xvf model.tar.gz -C model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Download a sample input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $s3_shapenet_uri/shapenet/raw/02691156/1021a0914a7207aff927ed529ad90a11.txt test_inf.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!head test_inf.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Inference Container\n",
    "\n",
    "You can test the inference container by running an interactive docker container on port 8080 and attaching the trained model.\n",
    "\n",
    "```\n",
    "$ docker run --rm -p 8080:8080 --mount src=\"$(pwd)/model\",target=\"/opt/ml/model\",type=bind sagemaker-torchpoints3d-inference:latest serve\n",
    "```\n",
    "\n",
    "Once your model is running you can, check the ping response:\n",
    "\n",
    "```\n",
    "$ curl localhost:8080/ping\n",
    "{\n",
    "  \"status\": \"Healthy\"\n",
    "}\n",
    "```\n",
    "\n",
    "Then post a request to the invocation endpoint with the sample file\n",
    "\n",
    "```\n",
    "$ curl -X POST http://localhost:9090/predictions/model -T test_inf.txt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy model\n",
    "\n",
    "Deploy the model for real-time inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "sm_client = boto3.client(service_name='sagemaker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import model into hosting\n",
    "\n",
    "First we create a model with the inference container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.utils import name_from_image\n",
    "\n",
    "inference_image = '{}.dkr.ecr.{}.amazonaws.com/sagemaker-torchpoints3d-inference:latest'.format(account_id, region)\n",
    "\n",
    "# Get a endpoint name based on the image\n",
    "endpoint_name = name_from_image(inference_image)\n",
    "\n",
    "container = {\n",
    "    'Image': inference_image,\n",
    "    'ModelDataUrl': estimator.model_data\n",
    "}\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName = endpoint_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    Containers = [container])\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response['ModelArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create endpoint configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = endpoint_name + 'EPConf'\n",
    "print('Endpoint config name: ' + endpoint_config_name)\n",
    "\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType': 'ml.c5.xlarge',\n",
    "        'InitialInstanceCount': 1,\n",
    "        'InitialVariantWeight': 1,\n",
    "        'ModelName': endpoint_name,\n",
    "        'VariantName': 'AllTraffic'}])\n",
    "\n",
    "print(\"Endpoint config Arn: \" + create_endpoint_config_response['EndpointConfigArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "endpoint_name = endpoint_name\n",
    "print('Endpoint name: ' + endpoint_name)\n",
    "\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name)\n",
    "print('Endpoint Arn: ' + create_endpoint_response['EndpointArn'])\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Endpoint Status: \" + status)\n",
    "\n",
    "print('Waiting for {} endpoint to be in service...'.format(endpoint_name))\n",
    "waiter = sm_client.get_waiter('endpoint_in_service')\n",
    "waiter.wait(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the input files into a byte array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'test_inf.txt'\n",
    "\n",
    "with open(filename, 'rb') as file:\n",
    "    body = file.read()\n",
    "    body = bytearray(body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform inference with the boto3 client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "import boto3\n",
    "\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName= endpoint_name,\n",
    "    Body= body,\n",
    "    ContentType = 'application/octet-stream')\n",
    "\n",
    "results = response['Body'].read()\n",
    "len(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize\n",
    "\n",
    "Load the results from prediction as numpy array and visualize with [mplot3d](https://matplotlib.org/mpl_toolkits/mplot3d/index.html) or iteratively in jupyter lab with [ipyvolume](https://ipyvolume.readthedocs.io/en/latest/install.html#for-jupyter-lab-users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipyvolume -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "data = np.array(json.loads(results)['response'])\n",
    "print(data.shape)\n",
    "\n",
    "data[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize with mplot3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "x,y,z,c = data[:,0], data[:,1], data[:,2], data[:,3]\n",
    "\n",
    "ax.scatter(x, y, z, c=c, marker='o')\n",
    "\n",
    "ax.set(xlim=(-0.4, 0.4), ylim=(-0.4, 0.4))\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "\n",
    "ax.view_init(elev=0., azim=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize point cloud in 3d ipvolume widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyvolume as ipv\n",
    "\n",
    "def get_coord(data, color):\n",
    "    mask = data[:,3]==color\n",
    "    return data[:,0][mask], data[:,1][mask], data[:,2][mask]\n",
    "\n",
    "fig = ipv.figure(width=600, height=600)\n",
    "\n",
    "x,y,z = get_coord(data, 6)\n",
    "scatter = ipv.scatter(x, y, z, size=1, marker='sphere', color='grey')\n",
    "\n",
    "x,y,z = get_coord(data, 7)\n",
    "scatter = ipv.scatter(x, y, z, size=1, marker='sphere', color='yellow')\n",
    "\n",
    "ipv.xyzlim(-0.5, 0.5)\n",
    "ipv.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
